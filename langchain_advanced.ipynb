{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xh13rSXo7HYx",
        "24llEWZl7LUX",
        "P2fB4gPYh9GW",
        "evpGRbcwJnXm",
        "oeAaaeupj6pn",
        "-asGPwz-prxX",
        "Hph_x__SpzHt",
        "-pARpvGdp9eq",
        "4lZTgcjNkhKK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharon220596/python/blob/main/langchain_advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "Xh13rSXo7HYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai langchain-core langchain-community python-dotenv\n",
        "!pip install faiss-cpu\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai')\n"
      ],
      "metadata": {
        "id": "1h155ku8qN8Z",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefd3867-1179-41c6-a3e4-cbdc0ba6f013"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.3-py3-none-any.whl (476 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.4/476.4 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.2.3 langchain-openai-1.1.6 langchain-text-splitters-1.1.0 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a simple call"
      ],
      "metadata": {
        "id": "24llEWZl7LUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the Prompt Template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that translates {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# LangChain automatically uses the API key from the environment variable\n",
        "openai = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "gemini = None\n",
        "# Initialize the Output Parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Build the LCEL Chain using the pipe (|) operator\n",
        "chain = prompt | openai | output_parser\n",
        "\n",
        "# Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"input_language\": \"English\",\n",
        "    \"output_language\": \"Hindi\",\n",
        "    \"text\": \"I love programming with LangChain!\"\n",
        "})\n",
        "\n",
        "\n",
        "print(result)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "cJyXaAUBIuae",
        "outputId": "dc173c19-043b-449d-a477-e224465167a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2973800190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Invoke the chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m result = chain.invoke({\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"input_language\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"English\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"output_language\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Hindi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         if (\n\u001b[1;32m   1382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m                 )\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SswHOk2gIsE3",
        "outputId": "4a67fa96-473b-49ba-d26d-50d13183a44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI 1st: Hello, Roopesh! How can I assist you today?\n",
            "AI 2nd: Your name is Roopesh! How can I help you today?\n",
            "\n",
            "Full conversation history: [HumanMessage(content=\"Hi, I'm Roopesh\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, Roopesh! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Roopesh! How can I help you today?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from google.colab import userdata # Assuming you are still in Colab\n",
        "\n",
        "# --- Setup (Install libraries if needed, set API key) ---\n",
        "# !pip install -qqq langchain-openai langchain-core langchain-community\n",
        "\n",
        "# 1. Model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 2. Prompt (Use MessagesPlaceholder for history injection)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a friendly assistant.\"),\n",
        "    # This placeholder handles the history correctly in v1.x\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# 3. Core Chain & Memory Store\n",
        "store = {} # A simple in-memory dictionary acting as our database\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# The core stateless chain built with LCEL\n",
        "core_chain = prompt | llm | output_parser\n",
        "\n",
        "# Function to provide the correct history object based on session ID\n",
        "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "# 4. Wrap the chain with the v1.x history manager\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    core_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\", # Key for the new human input\n",
        "    history_messages_key=\"chat_history\", # Key used in the prompt template placeholder\n",
        ")\n",
        "# 5. Interactions\n",
        "session_id = \"test_session_456\"\n",
        "config = {\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "# Interaction 1: Pass a dictionary with the \"input\" key\n",
        "r1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hi, I'm Roopesh\"}, # Corrected input format\n",
        "    config=config\n",
        ")\n",
        "print(f\"AI 1st: {r1}\")\n",
        "\n",
        "# Interaction 2: Pass a dictionary again\n",
        "r2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What's my name?\"}, # Corrected input format\n",
        "    config=config\n",
        ")\n",
        "print(f\"AI 2nd: {r2}\")\n",
        "\n",
        "# The store dictionary now holds the conversation history correctly\n",
        "print(\"\\nFull conversation history:\", store[session_id].messages)\n",
        "\n",
        "#Full conversation history: [HumanMessage(content=\"Hi, I'm Roopesh\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Roopesh! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Roopesh! How can I help you today?', additional_kwargs={}, response_metadata={})]\n",
        "\n",
        "#[26]\n",
        "#2s\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize components\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Single chained statement\n",
        "result = (\n",
        "    ChatPromptTemplate.from_template(\"Translate to German: {text}\") | llm | parser |\n",
        "    (lambda x: {\"text\": x}) |  # Wrap output as dict for next chain\n",
        "    ChatPromptTemplate.from_template(\"Make this exciting, but in simple language: {text}\") | llm | parser |\n",
        "    (lambda x: {\"text\": x}) |  # Wrap again\n",
        "    ChatPromptTemplate.from_template(\"Say this like actor Amitabh Bachchan: {text}\") | llm | parser\n",
        ").invoke({\"text\": \"The future of AI is exciting\"})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "WMkatsFZrpKK",
        "outputId": "3d4a62db-3834-45fa-d029-69834440594f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1755237233.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m  \u001b[0;31m# Wrap again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Say this like actor Amitabh Bachchan: {text}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m ).invoke({\"text\": \"The future of AI is exciting\"})\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m             cast(\n\u001b[1;32m    397\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    399\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1116\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 results.append(\n\u001b[0;32m--> 927\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    928\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         if (\n\u001b[1;32m   1382\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m                 )\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input_language\": \"English\",\n",
        "                       \"output_language\": \"Spanish\",\n",
        "                       \"text\": \"Hello\"})\n",
        "print(result)\n",
        "\n",
        "# 2. batch() - Multiple calls in parallel\n",
        "results = chain.batch([\n",
        "    {\"input_language\": \"English\", \"output_language\": \"French\", \"text\": \"Hello\"},\n",
        "    {\"input_language\": \"English\", \"output_language\": \"German\", \"text\": \"Hello\"},\n",
        "    {\"input_language\": \"English\", \"output_language\": \"Italian\", \"text\": \"Hello\"}\n",
        "])\n",
        "for r in results:\n",
        "    print(r)\n",
        "\n",
        "# 3. stream() - Token by token streaming\n",
        "for chunk in chain.stream({\"input_language\": \"English\",\n",
        "                           \"output_language\": \"Japanese\",\n",
        "                           \"text\": \"Hello\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "EfS49nPb2l8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b8c775-7a52-40ec-a240-680ab8592782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola\n",
            "Bonjour\n",
            "Hallo\n",
            "Ciao\n",
            "こんにちは (Konnichiwa)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    core_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")\n",
        "\n",
        "# 6. Have conversations!\n",
        "session_id = \"user_123\"\n",
        "config = {\"configurable\": {\"session_id\": session_id}}\n",
        "\n",
        "# First interaction\n",
        "response1 = chain_with_history.invoke(\n",
        "    {\"input\": \"Hi, I'm Roopesh\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"AI: {response1}\")\n",
        "# Output: \"Hello Roopesh! Nice to meet you. How can I help you today?\"\n",
        "\n",
        "# Second interaction - AI remembers!\n",
        "response2 = chain_with_history.invoke(\n",
        "    {\"input\": \"What's my name?\"},\n",
        "    config=config\n",
        ")\n",
        "print(f\"AI: {response2}\")\n",
        "# Output: \"Your name is Roopesh!\"\n",
        "\n",
        "# Check stored history\n",
        "print(\"\\nHistory:\", store[session_id].messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OVy7DsnWar3",
        "outputId": "213d6261-2319-4635-afb2-3fd1bb5fafdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Hello, Roopesh! How can I assist you today?\n",
            "AI: Your name is Roopesh. How can I help you today?\n",
            "\n",
            "History: [HumanMessage(content=\"Hi, I'm Roopesh\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello, Roopesh! How can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Roopesh. How can I help you today?', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# 1. Load and split documents\n",
        "loader = TextLoader(\"company_policies.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# 2. Create vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "u8IQxr_3WrA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer based on the following context:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# 4. Helper function to format docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 5. Build the RAG chain with LCEL\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableParallel({\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    })\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6. Ask questions!\n",
        "answer = rag_chain.invoke(\"What is the remote work policy?\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTGxmX_eXBYY",
        "outputId": "eee84e4f-54e0-4987-af6d-9842e125fda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The remote work policy allows employees to work remotely for two days per week, choosing any two days they prefer. Additionally, women are permitted to work remotely for three days per week.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from langgraph.prebuilt import create_react_agent\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a city.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "# Create agent with tools\n",
        "agent = create_agent(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH0FT1bbXp7r",
        "outputId": "2bbdcb57-435f-4f5f-fcaa-9a4434e70a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='90cbadc5-c799-4ea4-9d22-c2fcaca83277'),\n",
              " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 50, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj3CdoNpUNIKw3pCzYaRJO1zqRPLT', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--f4ff3f0e-34cd-4447-b7d4-2312899339c7-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_IAr1G6IqIV4TKtGUbeCigBcz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 50, 'output_tokens': 15, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='3506d43e-fbe8-49af-a8f6-e70bfa0705d7', tool_call_id='call_IAr1G6IqIV4TKtGUbeCigBcz'),\n",
              " AIMessage(content='The weather in San Francisco is sunny!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 81, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj3CdiKaYd1TIq1xlc1AMr4qeWF6l', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--b3430a19-8ae9-47d6-a2fa-d447bcee5352-0', usage_metadata={'input_tokens': 81, 'output_tokens': 9, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "P2fB4gPYh9GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# 1. Load and split documents\n",
        "loader = TextLoader(\"company_policies.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# 2. Create vector store\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "\n",
        "# 3. Define prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer based on the following context:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# 4. Helper function to format docs\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 5. Build the RAG chain with LCEL\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableParallel({\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    })\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6. Ask questions!\n",
        "#answer = rag_chain.invoke(\"What is the remote work policy?\")\n",
        "#print(answer)\n",
        "\n",
        "# Stream the response token by token, Output streams live as it's generated!\n",
        "for chunk in rag_chain.stream(\"What is the remote work policy?\"):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "#How many leaves can be availed by an employee?\n"
      ],
      "metadata": {
        "id": "hSYSJBdyYP70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770ada93-dd94-4096-987d-84f19a2c5f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The remote work policy at Ojasa Mirai LLP allows employees to work remotely up to 2 days per week with manager approval. Remote work days must be scheduled in advance through the HR portal. Additionally, women are permitted a total of 10 days of work-from-home, not exceeding 3 days in a week."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "l6NSX8hOiEo1",
        "outputId": "278be378-2fff-45a8-ea70-cc21e2f1a80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The remote work policy allows employees to work remotely up to 2 days per week with manager approval. Remote work days must be scheduled in advance through the HR portal. Additionally, women are permitted a total of 10 days of work-from-home, but cannot exceed 3 days in a single week.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lang Graph - Agent"
      ],
      "metadata": {
        "id": "i0Z2aH2VlEZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph ReAct Agent with OpenAI\n",
        "# This example shows how to create a simple agent that can use tools\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Define a tool using the @tool decorator\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a city. Use this when user asks about weather.\"\"\"\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "# Create the OpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Create agent with tools\n",
        "# Note: The prompt parameter is used for system message\n",
        "agent = create_react_agent(\n",
        "    model=model,\n",
        "    tools=[get_weather]\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "print(\"=\" * 60)\n",
        "print(\"LANGGRAPH REACT AGENT DEMO WITH OPENAI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]\n",
        "})\n",
        "\n",
        "# Print the conversation\n",
        "print(\"\\nConversation:\")\n",
        "print(\"-\" * 60)\n",
        "for message in result[\"messages\"]:\n",
        "    if hasattr(message, 'content'):\n",
        "        role = message.__class__.__name__\n",
        "        print(f\"\\n{role}:\")\n",
        "        print(message.content)\n",
        "    if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "        print(f\"\\nTool Calls: {message.tool_calls}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9D2514GlJOV",
        "outputId": "e30df9cc-7cc1-4557-8ded-cb572a06eb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2300434243.py:19: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LANGGRAPH REACT AGENT DEMO WITH OPENAI\n",
            "============================================================\n",
            "\n",
            "Conversation:\n",
            "------------------------------------------------------------\n",
            "\n",
            "HumanMessage:\n",
            "what is the weather in sf\n",
            "\n",
            "AIMessage:\n",
            "\n",
            "\n",
            "Tool Calls: [{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_wEBZBP4fyPmPUJDi7U1qGHUc', 'type': 'tool_call'}]\n",
            "\n",
            "ToolMessage:\n",
            "It's always sunny in San Francisco!\n",
            "\n",
            "AIMessage:\n",
            "The weather in San Francisco is always sunny!\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try more examples\n",
        "print(\"\\n\\nMORE EXAMPLES:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "examples = [\n",
        "    \"What's the weather in New York?\",\n",
        "    \"Compare weather in Paris and London\",\n",
        "    \"Tell me about the weather in Tokyo\"\n",
        "]\n",
        "\n",
        "for question in examples:\n",
        "    print(f\"\\n\\nUser: {question}\")\n",
        "    result = agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
        "    })\n",
        "    # Get the final response\n",
        "    final_message = result[\"messages\"][-1]\n",
        "    print(f\"Agent: {final_message.content}\")"
      ],
      "metadata": {
        "id": "-RIhWla8lMDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other ex"
      ],
      "metadata": {
        "id": "evpGRbcwJnXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt Template - be very specific to avoid web search explanations\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate the text from {input_language} to {output_language}. Return ONLY the translated text, no explanations or additional information.\"),\n",
        "    (\"user\", \"{text}\")\n",
        "])\n",
        "\n",
        "# Langchain automatically uses the API key from the environment variable\n",
        "\n",
        "# initialize the output parser\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Build the LCEL chain using the (|) pipe operator\n",
        "chain = prompt | openai | output_parser\n",
        "\n",
        "# Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"input_language\" : \"English\",\n",
        "    \"output_language\": \"Spanish\",\n",
        "    \"text\" : \"Hello , How is it going?\"\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOB1hG8YJrWF",
        "outputId": "201855a0-61c8-452c-9b05-438a233f938d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola, ¿cómo estás?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-agent"
      ],
      "metadata": {
        "id": "z3L6kAih2Ywr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.4)\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "research_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a research agent. Study the given topic and produce crisp bullet insights.\"),\n",
        "    (\"human\", \"Topic: {topic}\")\n",
        "])\n",
        "research_chain = research_prompt | model | output_parser\n",
        "\n",
        "writer_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a writing agent. Draft a clean and well-structured article based only on the research findings. Avoid repetition and fluff.\"),\n",
        "    (\"human\", \"Research findings:\\n{research_data}\")\n",
        "])\n",
        "writer_chain = writer_prompt | model | output_parser\n",
        "\n",
        "def orchestrate(topic: str):\n",
        "    research_summary = research_chain.invoke({\"topic\": topic})\n",
        "    article = writer_chain.invoke({\"research_data\": research_summary})\n",
        "    return {\"research\": research_summary, \"article\": article}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = \"Future of AI in retail inventory and automated logistics\"\n",
        "    result = orchestrate(topic)\n",
        "    print(\"\\n================ RESEARCH OUTPUT ================\\n\")\n",
        "    print(result[\"research\"])\n",
        "    print(\"\\n================ FINAL ARTICLE ================\\n\")\n",
        "    print(result[\"article\"])\n"
      ],
      "metadata": {
        "id": "a7kfqKWA2aWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents import create_agent, create_\n",
        "from langchain_core.tools import tool\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.4)\n",
        "\n",
        "@tool\n",
        "def pull_from_history(topic:str) -> str:\n",
        "  pass\n",
        "\n",
        "@tool\n",
        "def research(topic: str) -> str:\n",
        "    \"\"\"This is for research agent\"\"\"\n",
        "    # Create the prompt with proper formatting\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a research agent, please get the facts in detailed text under 500 words.\"),\n",
        "        (\"human\", \"research topic: {input_topic}\")\n",
        "    ])\n",
        "\n",
        "    # Format the prompt with the topic\n",
        "    prompt = prompt_template.format_messages(input_topic=topic)\n",
        "\n",
        "    # Call the model\n",
        "    output = model.invoke(prompt)\n",
        "    return output.content if hasattr(output, 'content') else str(output)\n",
        "\n",
        "@tool\n",
        "def writing(research_data: str) -> str:\n",
        "    \"\"\"This is for writing agent\"\"\"\n",
        "    # Create the prompt template\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a tech writer. Write the technical article in a national newspaper in India under 100 words.\"),\n",
        "        (\"human\", \"Please stick to the Research findings:\\n{input_data}\")\n",
        "    ])\n",
        "\n",
        "    # Format the prompt with the research data\n",
        "    prompt = prompt_template.format_messages(input_data=research_data)\n",
        "\n",
        "    # Call the model\n",
        "    output = model.invoke(prompt)\n",
        "    return output.content if hasattr(output, 'content') else str(output)\n",
        "\n",
        "agent_research=create_agent(\n",
        "    model=model,\n",
        "    tools=[research, pull_from_history]\n",
        ")\n",
        "agent_writer=create_agent(\n",
        "    model=model,\n",
        "    tools=[writing]\n",
        ")\n",
        "\n",
        "topic = {\"human\": \"increased influence of AI in education\"}\n",
        "output = agent_research.invoke(topic)\n",
        "print (output)\n",
        "## Orchestration\n"
      ],
      "metadata": {
        "id": "v8x1ngcn2au8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "921fe559-5105-4241-f49b-552acaacbcb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "create_agent() got an unexpected keyword argument 'llm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3653552854.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m agent_research=create_agent(\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresearch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_agent() got an unexpected keyword argument 'llm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmr53qHDAm1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-agent orchestrator\n",
        "\n",
        "Multi-Agent System Practice Code - LangChain/LangGraph\n",
        "=======================================================\n",
        "\n",
        "This code demonstrates a complete multi-agent system for content creation,\n",
        "following the supervisor/orchestrator pattern with specialized agents.\n",
        "\n",
        "Business Context: Content Creation Pipeline\n",
        "- Research Agent: Gathers information and sources\n",
        "- Outline Agent: Creates structured content outline\n",
        "- Writing Agent: Drafts the actual content\n",
        "- Editor Agent: Reviews and improves quality\n",
        "- Coordinator Agent: Orchestrates the entire workflow\n",
        "\n",
        "Based on the slide content: Multi-Agent Design"
      ],
      "metadata": {
        "id": "90Spej9mjWje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "kwJovS8RjbEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from typing import TypedDict, Annotated, List, Dict, Any\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.tools import tool, ToolRuntime\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# Initialize LLM with appropriate model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n"
      ],
      "metadata": {
        "id": "kevJpORjjfMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "oeAaaeupj6pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def web_search_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulates web search to gather information.\n",
        "    \"\"\"\n",
        "    # Simulated search results\n",
        "    search_results = {\n",
        "        \"AI\": \"AI (Artificial Intelligence) refers to systems that can perform tasks requiring human intelligence. Key developments include machine learning, neural networks, and large language models.\",\n",
        "        \"climate\": \"Climate change refers to long-term shifts in temperatures and weather patterns. Main causes include greenhouse gas emissions from fossil fuels.\",\n",
        "        \"technology\": \"Technology encompasses tools, systems, and methods used to solve problems. Recent trends include AI, cloud computing, and IoT.\",\n",
        "        \"default\": f\"Information about {query}: This is a simulated search result. In production, this would return real web data.\"\n",
        "    }\n",
        "\n",
        "    # Simple keyword matching for simulation\n",
        "    for keyword, result in search_results.items():\n",
        "        if keyword.lower() in query.lower():\n",
        "            return result\n",
        "\n",
        "    return search_results[\"default\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def fact_checker_tool(statement: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Checks facts and provides verification status.\n",
        "    In production, this would use fact-checking APIs.\n",
        "\n",
        "    Args:\n",
        "        statement: The statement to verify\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with verification status and details\n",
        "    \"\"\"\n",
        "    # Simulated fact checking\n",
        "    return {\n",
        "        \"statement\": statement,\n",
        "        \"verified\": True,\n",
        "        \"confidence\": 0.85,\n",
        "        \"sources\": [\"Simulated Source 1\", \"Simulated Source 2\"],\n",
        "        \"notes\": \"This is a simulated fact check. In production, use real verification services.\"\n",
        "    }\n",
        "\n",
        "\n",
        "@tool\n",
        "def grammar_checker_tool(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Checks grammar and style in the provided text.\n",
        "    In production, this would use services like Grammarly API.\n",
        "\n",
        "    Args:\n",
        "        text: The text to check\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with grammar check results\n",
        "    \"\"\"\n",
        "    # Simulated grammar checking\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    return {\n",
        "        \"text_length\": len(text),\n",
        "        \"word_count\": word_count,\n",
        "        \"issues_found\": 0,  # Simulated\n",
        "        \"readability_score\": 8.5,  # Simulated (out of 10)\n",
        "        \"suggestions\": [\"Great job! Text appears well-written.\"],\n",
        "        \"status\": \"approved\"\n",
        "    }\n",
        "\n",
        "\n",
        "@tool\n",
        "def outline_generator_tool(topic: str, research_data: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a structured outline based on research data.\n",
        "\n",
        "    Args:\n",
        "        topic: The main topic\n",
        "        research_data: Research information gathered\n",
        "\n",
        "    Returns:\n",
        "        Structured outline as a string\n",
        "    \"\"\"\n",
        "    outline = f\"\"\"\n",
        "CONTENT OUTLINE: {topic}\n",
        "\n",
        "I. Introduction\n",
        "   - Hook: Engaging opening about {topic}\n",
        "   - Context: Why this matters\n",
        "   - Thesis: Main argument\n",
        "\n",
        "II. Background & Research\n",
        "   {research_data[:200]}...\n",
        "\n",
        "III. Main Points\n",
        "   - Point 1: Key concept\n",
        "   - Point 2: Supporting evidence\n",
        "   - Point 3: Practical implications\n",
        "\n",
        "IV. Conclusion\n",
        "   - Summary of key points\n",
        "   - Call to action\n",
        "   - Future outlook\n",
        "\"\"\"\n",
        "    return outline\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0XB5067ej2uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "ZZ_YDI8PkBKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Research Agent"
      ],
      "metadata": {
        "id": "p51pDusRpftw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"\n",
        "    Agent 1: Research Agent\n",
        "    Responsibility: Searches for information and gathers sources\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.tools = [web_search_tool]\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a Research Agent specialized in gathering information.\n",
        "\n",
        "Your tasks:\n",
        "1. Use the web_search_tool to find relevant information\n",
        "2. Identify key points and facts\n",
        "3. Organize findings in a clear, structured way\n",
        "4. Cite sources when available\n",
        "\n",
        "Always be thorough and accurate in your research.\"\"\"),\n",
        "            (\"human\", \"{task}\")\n",
        "        ])\n",
        "\n",
        "    def execute(self, task: str) -> str:\n",
        "        \"\"\"Execute research task\"\"\"\n",
        "        # First, use the search tool\n",
        "        search_query = f\"research information about: {task}\"\n",
        "        search_results = web_search_tool.invoke({\"query\": search_query})\n",
        "\n",
        "        # Then, have the LLM process and organize the results\n",
        "        chain = self.prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        enhanced_task = f\"\"\"\n",
        "Task: {task}\n",
        "\n",
        "Search Results:\n",
        "{search_results}\n",
        "\n",
        "Please organize these findings into a clear research summary with key points.\n",
        "\"\"\"\n",
        "\n",
        "        result = chain.invoke({\"task\": enhanced_task})\n",
        "        return result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RqLrUlfgplB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OutlineAgent"
      ],
      "metadata": {
        "id": "-asGPwz-prxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlineAgent:\n",
        "    \"\"\"\n",
        "    Agent 2: Outline Agent\n",
        "    Responsibility: Creates structured content outline from research\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.tools = [outline_generator_tool]\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an Outline Agent specialized in creating structured content.\n",
        "\n",
        "Your tasks:\n",
        "1. Analyze research data\n",
        "2. Create logical content structure\n",
        "3. Organize main points hierarchically\n",
        "4. Ensure flow and coherence\n",
        "\n",
        "Create clear, actionable outlines that guide content creation.\"\"\"),\n",
        "            (\"human\", \"{research_data}\\n\\nTopic: {topic}\")\n",
        "        ])\n",
        "\n",
        "    def execute(self, topic: str, research_data: str) -> str:\n",
        "        \"\"\"Execute outline creation\"\"\"\n",
        "        # Use the outline generator tool\n",
        "        outline = outline_generator_tool.invoke({\n",
        "            \"topic\": topic,\n",
        "            \"research_data\": research_data\n",
        "        })\n",
        "\n",
        "        # Have the LLM enhance it\n",
        "        chain = self.prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        result = chain.invoke({\n",
        "            \"topic\": topic,\n",
        "            \"research_data\": f\"Research:\\n{research_data}\\n\\nInitial Outline:\\n{outline}\"\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n"
      ],
      "metadata": {
        "id": "xZcErXbzpxPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WritingAgent"
      ],
      "metadata": {
        "id": "Hph_x__SpzHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WritingAgent:\n",
        "    \"\"\"\n",
        "    Agent 3: Writing Agent\n",
        "    Responsibility: Drafts content based on outline and research\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a Writing Agent specialized in creating engaging content.\n",
        "\n",
        "Your tasks:\n",
        "1. Follow the provided outline structure\n",
        "2. Incorporate research findings naturally\n",
        "3. Write in a clear, engaging style\n",
        "4. Maintain consistent tone and voice\n",
        "5. Create smooth transitions between sections\n",
        "\n",
        "Write content that is informative, engaging, and well-structured.\"\"\"),\n",
        "            (\"human\", \"\"\"Create content based on:\n",
        "\n",
        "OUTLINE:\n",
        "{outline}\n",
        "\n",
        "RESEARCH DATA:\n",
        "{research_data}\n",
        "\n",
        "Topic: {topic}\"\"\")\n",
        "        ])\n",
        "\n",
        "    def execute(self, topic: str, outline: str, research_data: str) -> str:\n",
        "        \"\"\"Execute content writing\"\"\"\n",
        "        chain = self.prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        result = chain.invoke({\n",
        "            \"topic\": topic,\n",
        "            \"outline\": outline,\n",
        "            \"research_data\": research_data\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class EditorAgent:\n",
        "    \"\"\"\n",
        "    Agent 4: Editor Agent\n",
        "    Responsibility: Reviews and improves content quality\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.tools = [grammar_checker_tool, fact_checker_tool]\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an Editor Agent specialized in quality assurance.\n",
        "\n",
        "Your tasks:\n",
        "1. Check grammar and style\n",
        "2. Verify facts and accuracy\n",
        "3. Improve clarity and readability\n",
        "4. Ensure consistency\n",
        "5. Provide constructive feedback\n",
        "\n",
        "Be thorough but constructive in your review.\"\"\"),\n",
        "            (\"human\", \"\"\"Review this content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Provide:\n",
        "1. Quality assessment\n",
        "2. Suggestions for improvement\n",
        "3. Final recommendation (approve/revise)\"\"\")\n",
        "        ])\n",
        "\n",
        "    def execute(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"Execute content review\"\"\"\n",
        "        # Use grammar checker\n",
        "        grammar_check = grammar_checker_tool.invoke({\"text\": content})\n",
        "\n",
        "        # Use fact checker on key claims (simplified for demo)\n",
        "        # In production, you'd extract claims and check each\n",
        "\n",
        "        # Have LLM provide editorial review\n",
        "        chain = self.prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        editorial_review = chain.invoke({\"content\": content})\n",
        "\n",
        "        return {\n",
        "            \"grammar_check\": grammar_check,\n",
        "            \"editorial_review\": editorial_review,\n",
        "            \"status\": grammar_check[\"status\"],\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "MdIINu-xp6Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### COORDINATOR/ORCHESTRATOR AGENT"
      ],
      "metadata": {
        "id": "-pARpvGdp9eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoordinatorAgent:\n",
        "    \"\"\"\n",
        "    Main Coordinator Agent (Supervisor Pattern)\n",
        "    Responsibility: Orchestrates the entire multi-agent workflow\n",
        "\n",
        "    This implements the \"Tool Calling\" pattern from LangChain where\n",
        "    the coordinator treats other agents as tools to invoke.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "\n",
        "        # Initialize all specialized agents\n",
        "        self.research_agent = ResearchAgent(llm)\n",
        "        self.outline_agent = OutlineAgent(llm)\n",
        "        self.writing_agent = WritingAgent(llm)\n",
        "        self.editor_agent = EditorAgent(llm)\n",
        "\n",
        "        # Track workflow state\n",
        "        self.workflow_state = {}\n",
        "\n",
        "    def orchestrate_content_creation(self, topic: str) -> Dict[str, Any]:\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"🤖 MULTI-AGENT CONTENT CREATION PIPELINE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Topic: {topic}\\n\")\n",
        "\n",
        "        workflow_results = {\n",
        "            \"topic\": topic,\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"stages\": {}\n",
        "        }\n",
        "\n",
        "        # STAGE 1: Research\n",
        "        print(\"📚 Stage 1: Research Agent - Gathering Information...\")\n",
        "        research_data = self.research_agent.execute(topic)\n",
        "        workflow_results[\"stages\"][\"research\"] = {\n",
        "            \"agent\": \"ResearchAgent\",\n",
        "            \"output\": research_data,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        print(f\"✓ Research completed ({len(research_data)} characters)\\n\")\n",
        "\n",
        "        # STAGE 2: Outline\n",
        "        print(\"📝 Stage 2: Outline Agent - Creating Structure...\")\n",
        "        outline = self.outline_agent.execute(topic, research_data)\n",
        "        workflow_results[\"stages\"][\"outline\"] = {\n",
        "            \"agent\": \"OutlineAgent\",\n",
        "            \"output\": outline,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        print(f\"✓ Outline created\\n\")\n",
        "\n",
        "        # STAGE 3: Writing\n",
        "        print(\"✍️  Stage 3: Writing Agent - Drafting Content...\")\n",
        "        content = self.writing_agent.execute(topic, outline, research_data)\n",
        "        workflow_results[\"stages\"][\"writing\"] = {\n",
        "            \"agent\": \"WritingAgent\",\n",
        "            \"output\": content,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        print(f\"✓ Content drafted ({len(content)} characters)\\n\")\n",
        "\n",
        "        # STAGE 4: Editing\n",
        "        print(\"🔍 Stage 4: Editor Agent - Quality Review...\")\n",
        "        editor_results = self.editor_agent.execute(content)\n",
        "        workflow_results[\"stages\"][\"editing\"] = {\n",
        "            \"agent\": \"EditorAgent\",\n",
        "            \"output\": editor_results,\n",
        "            \"status\": \"completed\"\n",
        "        }\n",
        "        print(f\"✓ Editorial review completed\\n\")\n",
        "\n",
        "        # Final Results\n",
        "        workflow_results[\"end_time\"] = datetime.now().isoformat()\n",
        "        workflow_results[\"final_content\"] = content\n",
        "        workflow_results[\"quality_check\"] = editor_results\n",
        "\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"✅ WORKFLOW COMPLETED SUCCESSFULLY\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return workflow_results\n",
        "\n",
        "    def print_results(self, results: Dict[str, Any]):\n",
        "        \"\"\"Pretty print the workflow results\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"\\nTopic: {results['topic']}\")\n",
        "        print(f\"Start Time: {results['start_time']}\")\n",
        "        print(f\"End Time: {results['end_time']}\")\n",
        "\n",
        "        print(\"\\n--- RESEARCH FINDINGS ---\")\n",
        "        print(results[\"stages\"][\"research\"][\"output\"])\n",
        "\n",
        "        print(\"\\n--- CONTENT OUTLINE ---\")\n",
        "        print(results[\"stages\"][\"outline\"][\"output\"])\n",
        "\n",
        "        print(\"\\n--- FINAL CONTENT ---\")\n",
        "        print(results[\"final_content\"])\n",
        "\n",
        "        print(\"\\n--- QUALITY CHECK ---\")\n",
        "        print(f\"Grammar Check: {results['quality_check']['grammar_check']}\")\n",
        "        print(f\"Editorial Review:\\n{results['quality_check']['editorial_review']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "def example_2_full_pipeline():\n",
        "    \"\"\"Example 2: Full Content Creation Pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXAMPLE 2: Full Multi-Agent Content Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    coordinator = CoordinatorAgent(llm)\n",
        "    results = coordinator.orchestrate_content_creation(\n",
        "        \"The Impact of AI on Modern Education\"\n",
        "    )\n",
        "\n",
        "    coordinator.print_results(results)\n",
        "\n",
        "\n",
        "def example_3_custom_workflow():\n",
        "    \"\"\"Example 3: Custom workflow with specific agents\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"EXAMPLE 3: Custom Workflow - Research + Writing Only\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Use only specific agents\n",
        "    research_agent = ResearchAgent(llm)\n",
        "    writing_agent = WritingAgent(llm)\n",
        "\n",
        "    topic = \"Sustainable Energy Solutions\"\n",
        "\n",
        "    print(f\"\\n📚 Researching: {topic}\")\n",
        "    research = research_agent.execute(topic)\n",
        "\n",
        "    print(f\"\\n✍️  Writing content...\")\n",
        "    # Create a simple outline for the writer\n",
        "    simple_outline = f\"\"\"\n",
        "    1. Introduction to {topic}\n",
        "    2. Current challenges\n",
        "    3. Innovative solutions\n",
        "    4. Conclusion\n",
        "    \"\"\"\n",
        "\n",
        "    content = writing_agent.execute(topic, simple_outline, research)\n",
        "\n",
        "    print(\"\\n--- FINAL CONTENT ---\")\n",
        "    print(content)\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5JU6EWpekDP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "YVck58CfkaLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run examples\n",
        "try:\n",
        "\n",
        "    # Example 2: Full pipeline (production-like)\n",
        "    example_2_full_pipeline()\n",
        "\n",
        "    # Example 3: Custom workflow\n",
        "    #example_3_custom_workflow()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Error: {e}\")\n",
        "    print(\"\\nMake sure you have:\")\n",
        "    print(\"1. Set OPENAI_API_KEY environment variable\")\n",
        "    print(\"2. Installed required packages: pip install langchain langchain-openai\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixYXslnekb43",
        "outputId": "62a30800-31e3-4643-cd5e-79fff8dfd9d9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EXAMPLE 2: Full Multi-Agent Content Pipeline\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "🤖 MULTI-AGENT CONTENT CREATION PIPELINE\n",
            "======================================================================\n",
            "Topic: The Impact of AI on Modern Education\n",
            "\n",
            "📚 Stage 1: Research Agent - Gathering Information...\n",
            "✓ Research completed (1635 characters)\n",
            "\n",
            "📝 Stage 2: Outline Agent - Creating Structure...\n",
            "✓ Outline created\n",
            "\n",
            "✍️  Stage 3: Writing Agent - Drafting Content...\n",
            "✓ Content drafted (3456 characters)\n",
            "\n",
            "🔍 Stage 4: Editor Agent - Quality Review...\n",
            "✓ Editorial review completed\n",
            "\n",
            "======================================================================\n",
            "✅ WORKFLOW COMPLETED SUCCESSFULLY\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS\n",
            "======================================================================\n",
            "\n",
            "Topic: The Impact of AI on Modern Education\n",
            "Start Time: 2025-12-09T05:27:33.630069\n",
            "End Time: 2025-12-09T05:27:50.316968\n",
            "\n",
            "--- RESEARCH FINDINGS ---\n",
            "Research Summary:\n",
            "\n",
            "1. Definition of AI: AI, or Artificial Intelligence, encompasses systems capable of performing tasks that typically require human intelligence. This includes a range of technologies such as machine learning, neural networks, and large language models.\n",
            "\n",
            "2. Key Developments: In the realm of AI, significant advancements have been made in areas such as machine learning algorithms, neural network architectures, and the development of large language models. These innovations have paved the way for AI systems to learn from data, recognize patterns, and make decisions without explicit programming.\n",
            "\n",
            "3. Impact on Modern Education: The integration of AI in modern education has brought about transformative changes. AI-powered tools and platforms are being used to personalize learning experiences, provide real-time feedback to students, automate administrative tasks, and enhance the overall efficiency of educational processes.\n",
            "\n",
            "4. Potential Benefits: The utilization of AI in education has the potential to improve student outcomes by offering personalized learning pathways, identifying areas for improvement, and enabling educators to focus on individual student needs. Additionally, AI can help address challenges such as student engagement, retention, and access to quality education.\n",
            "\n",
            "5. Credible Sources: While specific sources were not mentioned in the search results, reputable sources for further information on the impact of AI on modern education could include academic journals, research papers, and reports from organizations focused on education technology and AI integration in learning environments.\n",
            "\n",
            "--- CONTENT OUTLINE ---\n",
            "Outline:\n",
            "\n",
            "I. Introduction\n",
            "   A. Hook: Illustrative scenario of AI application in education\n",
            "   B. Context: Explanation of AI and its significance in education\n",
            "   C. Thesis Statement: Highlighting the transformative impact of AI on modern education\n",
            "\n",
            "II. Understanding AI in Education\n",
            "   A. Definition of AI\n",
            "      1. Explanation of Artificial Intelligence\n",
            "      2. Components of AI in education (machine learning, neural networks, language models)\n",
            "   B. Key Developments in AI\n",
            "      1. Advancements in machine learning algorithms\n",
            "      2. Innovations in neural network architectures\n",
            "      3. Role of large language models\n",
            "\n",
            "III. Integration of AI in Modern Education\n",
            "   A. Transformative Changes in Education\n",
            "      1. Personalized learning experiences\n",
            "      2. Real-time feedback mechanisms\n",
            "      3. Automation of administrative tasks\n",
            "      4. Enhanced efficiency in educational processes\n",
            "   B. Impact on Student Outcomes\n",
            "      1. Personalized learning pathways\n",
            "      2. Identification of areas for improvement\n",
            "      3. Focus on individual student needs\n",
            "\n",
            "IV. Benefits and Challenges of AI in Education\n",
            "   A. Potential Benefits\n",
            "      1. Improved student outcomes\n",
            "      2. Addressing challenges in student engagement\n",
            "      3. Enhancing access to quality education\n",
            "   B. Challenges and Considerations\n",
            "      1. Ethical implications of AI in education\n",
            "      2. Equity and accessibility concerns\n",
            "      3. Teacher training and support for AI integration\n",
            "\n",
            "V. Credible Sources and Further Reading\n",
            "   A. Academic Journals\n",
            "   B. Research Papers\n",
            "   C. Reports from Education Technology Organizations\n",
            "\n",
            "VI. Conclusion\n",
            "   A. Summary of Key Points\n",
            "   B. Call to Action for Educators and Institutions\n",
            "   C. Future Outlook: Potential advancements and implications of AI in education\n",
            "\n",
            "--- FINAL CONTENT ---\n",
            "**Title: Revolutionizing Education: The Transformative Impact of Artificial Intelligence**\n",
            "\n",
            "**I. Introduction**\n",
            "\n",
            "A. *Hook*: Imagine a classroom where each student receives personalized learning experiences tailored to their unique needs, where real-time feedback guides their progress, and administrative tasks are seamlessly managed. This scenario is made possible by the integration of Artificial Intelligence (AI) in education.\n",
            "\n",
            "B. *Context*: AI refers to systems that can perform tasks requiring human intelligence, utilizing technologies like machine learning, neural networks, and large language models. Its significance in education lies in its ability to revolutionize traditional teaching methods and enhance learning outcomes.\n",
            "\n",
            "C. *Thesis Statement*: The transformative impact of AI on modern education is reshaping the way students learn, teachers instruct, and institutions operate, ushering in a new era of personalized, efficient, and effective educational practices.\n",
            "\n",
            "**II. Understanding AI in Education**\n",
            "\n",
            "A. *Definition of AI*: AI encompasses systems capable of mimicking human intelligence, utilizing machine learning, neural networks, and language models to process data, recognize patterns, and make decisions.\n",
            "\n",
            "B. *Key Developments in AI*: Recent advancements in machine learning algorithms, neural network architectures, and the rise of large language models have empowered AI to learn autonomously, leading to breakthroughs in various educational applications.\n",
            "\n",
            "**III. Integration of AI in Modern Education**\n",
            "\n",
            "A. *Transformative Changes in Education*: AI has revolutionized education by offering personalized learning experiences, real-time feedback mechanisms, automation of administrative tasks, and enhanced efficiency in educational processes.\n",
            "\n",
            "B. *Impact on Student Outcomes*: Students benefit from personalized learning pathways, targeted improvement strategies, and individualized attention, resulting in improved academic achievements and a focus on their unique learning needs.\n",
            "\n",
            "**IV. Benefits and Challenges of AI in Education**\n",
            "\n",
            "A. *Potential Benefits*: AI improves student outcomes, addresses challenges in engagement, and enhances access to quality education, providing a more inclusive and effective learning environment.\n",
            "\n",
            "B. *Challenges and Considerations*: Ethical implications, equity concerns, and the need for teacher training pose challenges to the widespread integration of AI in education, requiring careful consideration and strategic planning.\n",
            "\n",
            "**V. Credible Sources and Further Reading**\n",
            "\n",
            "A. *Academic Journals*: Explore in-depth research on AI's impact on education and learning outcomes.\n",
            "B. *Research Papers*: Access detailed studies on the effectiveness of AI tools in educational settings.\n",
            "C. *Reports from Education Technology Organizations*: Stay informed about the latest trends and innovations in AI integration in education.\n",
            "\n",
            "**VI. Conclusion**\n",
            "\n",
            "A. *Summary of Key Points*: AI's transformative impact on modern education is evident in its personalized learning approaches, improved student outcomes, and enhanced efficiency in educational processes.\n",
            "B. *Call to Action*: Educators and institutions are encouraged to embrace AI technologies to enhance teaching practices and student learning experiences.\n",
            "C. *Future Outlook*: As AI continues to evolve, the potential advancements and implications in education are vast, signaling a promising future for innovative and effective educational practices.\n",
            "\n",
            "--- QUALITY CHECK ---\n",
            "Grammar Check: {'text_length': 3456, 'word_count': 454, 'issues_found': 0, 'readability_score': 8.5, 'suggestions': ['Great job! Text appears well-written.'], 'status': 'approved'}\n",
            "Editorial Review:\n",
            "Quality Assessment:\n",
            "Overall, the content is well-structured and informative. The introduction effectively sets the stage for discussing the impact of AI in education, and the subsequent sections delve into key aspects such as understanding AI, integration in education, benefits, challenges, and credible sources. The language is clear and professional, maintaining a formal tone suitable for the topic.\n",
            "\n",
            "Suggestions for Improvement:\n",
            "1. The content could benefit from more specific examples or case studies to illustrate the points made about the impact of AI in education. This would make the information more tangible and engaging for the readers.\n",
            "2. Consider incorporating statistics or data to support the claims about the benefits of AI in education. This would enhance the credibility of the arguments presented.\n",
            "3. The conclusion could be strengthened by summarizing the potential risks or limitations of AI in education, providing a more balanced perspective on the topic.\n",
            "\n",
            "Final Recommendation:\n",
            "Overall, the content is well-written and provides a comprehensive overview of the transformative impact of AI in education. I recommend revising the content to include specific examples, statistical evidence, and a more balanced conclusion to further enhance the quality and depth of the discussion. Once these revisions are made, the content will be ready for approval.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trouble shooting\n",
        "\n"
      ],
      "metadata": {
        "id": "4lZTgcjNkhKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### more details here\n",
        "\"\"\"\n",
        "📚 KEY CONCEPTS DEMONSTRATED:\n",
        "\n",
        "1. MULTI-AGENT ARCHITECTURE\n",
        "   - Specialized agents with clear responsibilities\n",
        "   - Coordinator/Supervisor pattern\n",
        "   - Tool calling for agent orchestration\n",
        "\n",
        "2. CONTEXT ENGINEERING\n",
        "   - Each agent gets relevant context\n",
        "   - State passing between agents\n",
        "   - Tool integration with agents\n",
        "\n",
        "3. WORKFLOW ORCHESTRATION\n",
        "   - Sequential agent execution\n",
        "   - State management\n",
        "   - Error handling and validation\n",
        "\n",
        "⚠️ CHALLENGES IN MULTI-AGENT SYSTEMS (from slides):\n",
        "\n",
        "1. COST\n",
        "   - Multiple LLM calls increase expenses\n",
        "   - Each agent invocation = API cost\n",
        "   - Solution: Cache results, batch operations\n",
        "\n",
        "2. LATENCY\n",
        "   - Sequential agents = slower responses\n",
        "   - Network overhead for each call\n",
        "   - Solution: Parallel execution where possible\n",
        "\n",
        "3. ERROR PROPAGATION\n",
        "   - One agent's mistake affects downstream agents\n",
        "   - Solution: Validation at each stage, error handling\n",
        "\n",
        "4. DEBUGGING\n",
        "   - Complex to trace issues across agents\n",
        "   - Solution: Comprehensive logging, state tracking\n",
        "\n",
        "5. COORDINATION\n",
        "   - Agents need clear communication protocols\n",
        "   - Solution: Well-defined interfaces, state schemas\n",
        "\n",
        "🎯 WHEN TO USE MULTI-AGENT SYSTEMS:\n",
        "\n",
        "✅ Good Use Cases:\n",
        "   - Complex workflows (research → analysis → report)\n",
        "   - Need for specialization (legal + medical + financial)\n",
        "   - Quality control (agent reviews another's work)\n",
        "   - Debate & consensus (multiple perspectives)\n",
        "\n",
        "❌ Avoid When:\n",
        "   - Simple, single-step tasks\n",
        "   - Low latency requirements\n",
        "   - Budget constraints\n",
        "\n",
        "🔮 FUTURE DIRECTIONS:\n",
        "   - Autonomous agents with minimal intervention\n",
        "   - Collaborative problem-solving\n",
        "   - Specialized expert agents\n",
        "   - Self-improving agent teams\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UJHNXRaJl3Em"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples"
      ],
      "metadata": {
        "id": "oJKf9DBnuPPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculator agent\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "# HumanMessage\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# tool\n",
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def add(num1 , num2):\n",
        "  \"\"\"add two numbers\"\"\"\n",
        "  print (\"Add is called\")\n",
        "  return num1 + num2\n",
        "\n",
        "@tool\n",
        "def method1(num1 , num2):\n",
        "  \"\"\"subtract two numbers\"\"\"\n",
        "  print (\"method1 is called\")\n",
        "  return num1 + num2\n",
        "\n",
        "@tool\n",
        "def square(num1):\n",
        "  \"\"\"square a number\"\"\"\n",
        "  print (\"square is called\")\n",
        "  return num1 * num1\n",
        "\n",
        "def create_calc_agent (llm):\n",
        "  agent = create_agent(\n",
        "    tools = [add, square, method1],\n",
        "    model = llm\n",
        "  )\n",
        "  return agent\n",
        "\n",
        "agent = create_calc_agent (llm)\n",
        "\n",
        "result = agent.invoke({\"messages\": [HumanMessage( \"what is 3-2\")]})\n",
        "print (result['messages'][-1].content)\n",
        "#result = agent.invoke({\"messages\": [HumanMessage( \"what is square of 9\")]})\n",
        "#print(result['messages'][-1].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoMx-KRkuZ_B",
        "outputId": "85e16272-4161-42b7-ab9b-191614e43791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "method1 is called\n",
            "3 - 2 is equal to 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ88hQrYvhPK",
        "outputId": "0c617165-e7af-42ab-a60c-27cb0cd9932a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x797c8bac38f0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x797c8bb15580>, root_client=<openai.OpenAI object at 0x797c8cb49670>, root_async_client=<openai.AsyncOpenAI object at 0x797c8c0f0080>, temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculator agent\n",
        "\n",
        "@tool\n",
        "def add(num1 , num2)\n",
        "\n",
        "@tool\n",
        "def square(num1)\n",
        "\n"
      ],
      "metadata": {
        "id": "gyGYCc77toxr"
      }
    }
  ]
}